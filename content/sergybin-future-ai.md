---
title: Sergey Brin on Google AI Progress & Future
tags: [AI, Google, Gemini, VO, Diffusion, Training, IO, Brin]
description: Sergey Brin discusses Google I/O announcements, Gemini and VO models, training strategies, and the rapid pace of AI innovation.
markmap:
  colorFreezeLevel: 2
  maxWidth: 300
---

# Sergey Brin on Google AI Progress & Future

## Google I/O Announcements
- Phenomenal set of announcements
- Surprised by some features (e.g., virtual fit in Search)
- Many things to explore and understand
- Busy delivering all announced features

## Sergey Brin's Focus
- Mostly on Gemini core text model
  - Believes it leads to self-improvement
  - Helps in coding and developing AI science
- Relies on Gemini for coding and math

## Generative Media Models
- VO (Video with audio)
  - Superhuman capability compared to Brin's artistic talent
  - Audio adds significant richness and changes perception
  - Audio in V3 "blew my mind"
  - Vio likely uses diffusion for audio
- Imagine (Image generation)
- LIA (Music model)
- Gemini Diffusion (Text diffusion)
  - Promising results
- Suite of models pushing the frontier
- Technologically different from mainline Gemini audio support

## Model Training & Development
- Watching training runs
  - Testing intermediate checkpoints (e.g., 10%, 20% training)
  - Get sense of trajectory
  - Especially for big runs with high hopes
- Seen Vio model training checkpoints develop

## Surprises & Expectations
- Intellectually reasoning through singularity vs. seeing it happen
  - Ray Kurzweil's prediction (maybe conservative)
  - Seeing it happen feels very different
- The way it happens is surprising
  - Language models leading AI development wasn't obvious 15 years ago
  - DeepMind previously bet on physical grounding
- Language models are surprisingly interpretable
  - Can look through thoughts of thinking models
  - Provides comfort for safety (saying what they're thinking)

## Architectural Similarities
- Remarkable how architecturally similar different models are
  - Includes Vio (video diffusion) and text language models
- Huge amount shared
- Many use Transformers at the core

## Training Phases
- Increasing fraction of post-training (fine-tuning, RL work)
  - Used to be 99% pre-training
  - Shifting to 90%, 80% or less
- Tool use integration happens during post-training
- Makes models vastly more powerful

## Reasoning Scaling
- Multiple approaches converged on Deep Think
- Yielding stronger results
- Superpower if models can think longer (hour, day, month) for better answers
- Analogous to cracking long context for input
- Need "infinite context" (non-trivial generalization)
- Non-trivial gap being overcome
  - Training models on short tasks vs. expecting long-term development/thinking

## Evaluation Challenges
- Much of life is an eval problem (AI, interviewing people)
- Humans haven't solved human eval problem
- Not surprising AI eval problem isn't solved

## Google's Reinvention & Innovation Pace
- Companies need to periodically reinvent themselves
- AI domain is exciting
- Google always felt like a startup recently
- Google has AI in its DNA (large scale data, analysis, Google Brain, Transformer)
- Well-equipped for this shift
- Clear acceleration (2025 section bigger than 2024 on a slide)
- 2.5 Pro was a clear leap forward
  - Remains number one on many leaderboards
  - Cause and effect of science engine
- Quick succession of other things followed 2.5 Pro
- New 2.5 Flash launched (number two on many leaderboards)
  - Super fast, powerful, appeals to many use cases
- Momentum continues with 2.5 Pro as cornerstone

## TPUs
- TPU V4 (Pufferfish)
  - Hottest thing 1-2 years ago
  - Still used for a lot of work